{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_envs = 8\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "# PPO Parameter\n",
    "learning_rate = 3e-4\n",
    "n_rollout_steps = 1024\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "clip_range = 0.2\n",
    "normalize_advantage = True\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "buffer_size = n_envs * n_rollout_steps\n",
    "\n",
    "vec_env = [gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\") for _ in range(n_envs)]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((84, 84), antialias=None), transforms.Grayscale()])\n",
    "\n",
    "last_obs = torch.empty((n_envs, 4, 84, 84), dtype=torch.float32)\n",
    "for i, env in enumerate(vec_env):\n",
    "    obs, _, = env.reset()\n",
    "    last_obs[i, :] = transform(obs)\n",
    "\n",
    "episode_frame_numbers = []\n",
    "episode_rewards = []\n",
    "vec_env_reward = [0 for _ in range(n_envs)]\n",
    "def on_rollout_start():\n",
    "    episode_frame_numbers.clear()\n",
    "    episode_rewards.clear()\n",
    "\n",
    "def step(vec_env, actions):\n",
    "    buf_obs = torch.empty((n_envs, 4, 84, 84), dtype=torch.float32)\n",
    "    buf_rews = torch.zeros((n_envs,), dtype=torch.float32)\n",
    "    buf_done = torch.zeros((n_envs,), dtype=torch.bool)\n",
    "\n",
    "    for i in range(n_envs):\n",
    "        for j in range(4):\n",
    "            obs, rew, terminated, truncated, info = vec_env[i].step(actions[i])\n",
    "            buf_rews[i] += rew\n",
    "            vec_env_reward[i] += rew\n",
    "            if terminated or truncated:\n",
    "                buf_done[i] = True\n",
    "                obs, _, = vec_env[i].reset()\n",
    "                buf_obs[i, :] = transform(obs)\n",
    "\n",
    "                episode_frame_numbers.append(info[\"episode_frame_number\"])\n",
    "                episode_rewards.append(vec_env_reward[i])\n",
    "                vec_env_reward[i] = 0\n",
    "                break\n",
    "            buf_obs[i, j] = transform(obs)\n",
    "    return buf_obs, buf_rews, buf_done\n",
    "\n",
    "\n",
    "class RolloutBufferSamples(NamedTuple):\n",
    "    observations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    old_values: torch.Tensor\n",
    "    old_log_prob: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self, buffer_size, n_envs, obs_shape, action_dim, device):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_envs = n_envs\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "    def reset(self):\n",
    "        self.observations = torch.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=torch.float32)\n",
    "        self.actions = torch.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=torch.int64)\n",
    "        self.rewards = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.returns = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.episode_starts = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.values = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.log_probs = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.advantages = torch.zeros((self.buffer_size, self.n_envs), dtype=torch.float32)\n",
    "        self.pos = 0\n",
    "        self.generator_ready = False\n",
    "\n",
    "    def add(self, obs, action, reward, episode_start, value, log_prob):\n",
    "        self.observations[self.pos] = obs\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.episode_starts[self.pos] = episode_start\n",
    "        self.values[self.pos] = value.cpu().flatten()\n",
    "        self.log_probs[self.pos] = log_prob.cpu()\n",
    "        self.pos += 1\n",
    "\n",
    "    def compute_returns_and_advantage(self, last_values, dones):\n",
    "        last_values = last_values.cpu().flatten()\n",
    "\n",
    "        last_gae_lam = 0\n",
    "        for step in reversed(range(self.buffer_size)):\n",
    "            if step == self.buffer_size - 1:\n",
    "                next_non_terminal = 1.0 - dones.to(torch.float32)\n",
    "                next_values = last_values\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.episode_starts[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            delta = self.rewards[step] + gamma * next_values * next_non_terminal - self.values[step]\n",
    "            last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            self.advantages[step] = last_gae_lam\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    @staticmethod\n",
    "    def swap_and_flatten(arr):\n",
    "        shape = arr.shape\n",
    "        if len(shape) < 3:\n",
    "            shape = (*shape, 1)\n",
    "        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        indices = np.random.permutation(self.buffer_size * self.n_envs)\n",
    "\n",
    "        if not self.generator_ready:\n",
    "            self.observations = self.swap_and_flatten(self.observations)\n",
    "            self.actions = self.swap_and_flatten(self.actions)\n",
    "            self.values = self.swap_and_flatten(self.values)\n",
    "            self.log_probs = self.swap_and_flatten(self.log_probs)\n",
    "            self.advantages = self.swap_and_flatten(self.advantages)\n",
    "            self.returns = self.swap_and_flatten(self.returns)\n",
    "            self.generator_ready = True\n",
    "\n",
    "        start_idx = 0\n",
    "        while start_idx < self.buffer_size * self.n_envs:\n",
    "            yield self._get_samples(indices[start_idx : start_idx + batch_size])\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def to_torch(self, array):\n",
    "        return torch.as_tensor(array, device=self.device)\n",
    "\n",
    "    def _get_samples(\n",
    "        self,\n",
    "        batch_inds\n",
    "    ):\n",
    "        data = (\n",
    "            self.observations[batch_inds],\n",
    "            self.actions[batch_inds],\n",
    "            self.values[batch_inds].flatten(),\n",
    "            self.log_probs[batch_inds].flatten(),\n",
    "            self.advantages[batch_inds].flatten(),\n",
    "            self.returns[batch_inds].flatten(),\n",
    "        )\n",
    "        return RolloutBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(3136, 512)\n",
    "        self.fc_p = nn.Linear(512, 4)\n",
    "        self.fc_v = nn.Linear(512, 1)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc(x.flatten(1)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        policy = F.relu(self.fc_p(x))\n",
    "        value = F.relu(self.fc_v(x))\n",
    "        return policy, value\n",
    "    \n",
    "    def predict_values(self, x):\n",
    "        x = self.extract_features(x)\n",
    "        value = F.relu(self.fc_v(x))\n",
    "        return value\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_prob(value, logits):\n",
    "        value, log_pmf = torch.broadcast_tensors(value, logits)\n",
    "        value = value[..., :1]\n",
    "        log_prob = log_pmf.gather(-1, value).squeeze(-1)\n",
    "        return log_prob\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(logits):\n",
    "        min_real = torch.finfo(logits.dtype).min\n",
    "        logits = torch.clamp(logits, min=min_real)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        p_log_p = logits * probs\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        logits, values = self.forward(obs)\n",
    "        # Normalize\n",
    "        logits = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        actions = torch.multinomial(probs, 1, True)\n",
    "        return actions, values, self.log_prob(actions, logits)\n",
    "    \n",
    "    def evaluate_actions(self, obs, actions):\n",
    "        logits, values = self.forward(obs)\n",
    "        # Normalize\n",
    "        logits = logits - logits.logsumexp(dim=-1, keepdim=True)\n",
    "        log_prob = self.log_prob(actions, logits)\n",
    "        entropy = self.entropy(logits)\n",
    "        return values, log_prob, entropy\n",
    "    \n",
    "\n",
    "model = PolicyValueNet()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "rollout_buffer = RolloutBuffer(n_rollout_steps, n_envs, (4, 84, 84), 1, device)\n",
    "\n",
    "iteration = 0\n",
    "num_timesteps = 0\n",
    "global_step = 0\n",
    "while num_timesteps < total_timesteps:\n",
    "    last_episode_starts = torch.ones((n_envs,), dtype=torch.bool)\n",
    "\n",
    "    # collect_rollouts\n",
    "    model.eval()\n",
    "    n_steps = 0\n",
    "    rollout_buffer.reset()\n",
    "    on_rollout_start()\n",
    "    while n_steps < n_rollout_steps:\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = last_obs.to(device)\n",
    "            actions, values, log_probs = model.sample(obs_tensor)\n",
    "\n",
    "        actions = actions.cpu()\n",
    "        new_obs, rewards, dones = step(vec_env, actions.reshape(-1))\n",
    "        \n",
    "        num_timesteps += n_envs\n",
    "        n_steps += 1\n",
    "\n",
    "        rollout_buffer.add(\n",
    "            last_obs,\n",
    "            actions,\n",
    "            rewards,\n",
    "            last_episode_starts,\n",
    "            values,\n",
    "            log_probs,\n",
    "        )\n",
    "        last_obs = new_obs\n",
    "        last_episode_starts = dones\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute value for the last timestep\n",
    "        values = model.predict_values(new_obs.to(device))\n",
    "    # Compute the lambda-return (TD(lambda) estimate) and GAE(lambda) advantage\n",
    "    rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    # Logs\n",
    "    writer.add_scalar(\"rollout/ep_len_mean\", sum(episode_frame_numbers) / len(episode_frame_numbers), global_step)\n",
    "    writer.add_scalar(\"rollout/ep_rew_mean\", sum(episode_rewards) / len(episode_rewards), global_step)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for rollout_data in rollout_buffer.get(batch_size):\n",
    "            actions = rollout_data.actions\n",
    "\n",
    "            values, log_prob, entropy = model.evaluate_actions(rollout_data.observations, actions)\n",
    "            values = values.flatten()\n",
    "\n",
    "            # Normalize advantage\n",
    "            advantages = rollout_data.advantages\n",
    "            if normalize_advantage:\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # ratio between old and new policy, should be one at the first iteration\n",
    "            ratio = torch.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            policy_loss_1 = advantages * ratio\n",
    "            policy_loss_2 = advantages * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "            policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "            # Value loss using the TD(gae_lambda) target\n",
    "            value_loss = F.mse_loss(rollout_data.returns, values)\n",
    "\n",
    "            # Entropy loss favor exploration\n",
    "            entropy_loss = -torch.mean(entropy)\n",
    "\n",
    "            loss = policy_loss + ent_coef * entropy_loss + vf_coef * value_loss\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip grad norm\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logs\n",
    "            writer.add_scalar(\"train/policy_loss\", policy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/value_loss\", value_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/entropy_loss\", entropy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, \"checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

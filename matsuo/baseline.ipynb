{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkmur/anaconda3/envs/web_con/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from miniwob.action import ActionTypes\n",
    "from ppo.buffer import RolloutBuffer\n",
    "from ppo.network import PolicyValueNet\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_envs = 2\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "# PPO Parameter\n",
    "learning_rate = 3e-4\n",
    "n_rollout_steps = 512\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "clip_range = 0.2\n",
    "normalize_advantage = True\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "buffer_size = n_envs * n_rollout_steps\n",
    "\n",
    "OBS_SHAPE = (3, 234, 234)\n",
    "ACTION_DIM = 2 # coords\n",
    "\n",
    "vec_env = [gym.make('miniwob/click-test-2-v1', render_mode=None) for _ in range(n_envs)]\n",
    "def prep(obs):\n",
    "    # (210, 160, 3) -> (3, 234, 234)\n",
    "    # print(obs[\"screenshot\"].shape)\n",
    "    new_obs = np.transpose(obs[\"screenshot\"], (2, 0, 1))\n",
    "    new_obs = np.pad(new_obs, ((0, 0), (12, 12), (37, 37)), mode='constant', constant_values=0).astype(np.float32)\n",
    "    # print(new_obs.shape)\n",
    "    return torch.tensor(new_obs/255.)\n",
    "transform = transforms.Compose([prep])\n",
    "# model input\n",
    "last_obs1 = torch.empty((n_envs, *OBS_SHAPE), dtype=torch.float32)\n",
    "def create_action(env, action):\n",
    "    return env.unwrapped.create_action(ActionTypes.CLICK_COORDS, coords = action.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot call OrderedDict([('action_type', 2), ('coords', array([ 20., 116.], dtype=float32)), ('field', 5), ('key', 67), ('ref', 797562), ('text', 'QYntL1uPk0eFis4eR59z3P5VFcTIgtMU')]) on instance 0, which is already done\n"
     ]
    }
   ],
   "source": [
    "episode_frame_numbers = []\n",
    "episode_rewards = []\n",
    "vec_env_reward = [0 for _ in range(n_envs)]\n",
    "label = torch.empty((OBS_SHAPE[1]*OBS_SHAPE[2], 2))\n",
    "n=0\n",
    "for i in range(OBS_SHAPE[1]):\n",
    "    for j in range(OBS_SHAPE[2]):\n",
    "        label[n] = torch.tensor([i, j])\n",
    "        n+=1\n",
    "def on_rollout_start():\n",
    "    episode_frame_numbers.clear()\n",
    "    episode_rewards.clear()\n",
    "\n",
    "def step(vec_env, actions):\n",
    "    buf_obs1 = torch.empty((n_envs, *OBS_SHAPE), dtype=torch.float32)\n",
    "    buf_rews = torch.zeros((n_envs,), dtype=torch.float32)\n",
    "    buf_done = torch.zeros((n_envs,), dtype=torch.bool)\n",
    "    action_labels = torch.empty((actions.shape[0], 2))\n",
    "    for idx in range(actions.shape[0]):\n",
    "        action_labels[idx] = label[actions[idx].detach().cpu()]\n",
    "    for i in range(n_envs):\n",
    "        for j in range(4):\n",
    "            action = create_action(vec_env[i], action_labels[i])\n",
    "            # print(action, actions[i], actions)\n",
    "            obs, rew, terminated, truncated, info = vec_env[i].step(action)\n",
    "            buf_rews[i] += rew\n",
    "            vec_env_reward[i] += rew\n",
    "            if terminated or truncated:\n",
    "                buf_done[i] = True\n",
    "                obs, _, = vec_env[i].reset()\n",
    "                buf_obs1[i, :] = transform(obs)\n",
    "\n",
    "                # episode_frame_numbers.append(info[\"episode_frame_number\"])\n",
    "                episode_rewards.append(vec_env_reward[i])\n",
    "                vec_env_reward[i] = 0\n",
    "                break\n",
    "            buf_obs1[i,] = transform(obs)\n",
    "    return buf_obs1, buf_rews, buf_done\n",
    "\n",
    "model = PolicyValueNet(in_shape=OBS_SHAPE, out_shape=(1, OBS_SHAPE[1], OBS_SHAPE[2]))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "rollout_buffer = RolloutBuffer(n_rollout_steps, n_envs, OBS_SHAPE, 1, device)\n",
    "\n",
    "iteration = 0\n",
    "num_timesteps = 0\n",
    "global_step = 0\n",
    "\n",
    "for i, env in enumerate(vec_env):\n",
    "    obs, _, = env.reset()\n",
    "    # element = get_element(obs)\n",
    "    last_obs1[i, :] = transform(obs)\n",
    "    \n",
    "while num_timesteps < total_timesteps:\n",
    "    last_episode_starts = torch.ones((n_envs,), dtype=torch.bool)\n",
    "\n",
    "    # collect_rollouts\n",
    "    model.eval()\n",
    "    n_steps = 0\n",
    "    rollout_buffer.reset()\n",
    "    on_rollout_start()\n",
    "    while n_steps < n_rollout_steps:\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = last_obs1.to(device)\n",
    "            actions, values, log_probs = model.sample(obs_tensor)\n",
    "            \n",
    "        actions = actions.cpu()\n",
    "        \n",
    "        new_obs1, rewards, dones  = step(vec_env, actions)\n",
    "        \n",
    "        num_timesteps += n_envs\n",
    "        n_steps += 1\n",
    "\n",
    "        rollout_buffer.add(\n",
    "            last_obs1,\n",
    "            actions,\n",
    "            rewards,\n",
    "            last_episode_starts,\n",
    "            values,\n",
    "            log_probs,\n",
    "        )\n",
    "        last_obs1 = new_obs1\n",
    "        last_episode_starts = dones\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute value for the last timestep\n",
    "        values = model.predict_values(new_obs1.to(device))\n",
    "    # Compute the lambda-return (TD(lambda) estimate) and GAE(lambda) advantage\n",
    "    rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    # Logs\n",
    "    # writer.add_scalar(\"rollout/ep_len_mean\", sum(episode_frame_numbers) / len(episode_frame_numbers), global_step)\n",
    "    writer.add_scalar(\"rollout/ep_rew_mean\", sum(episode_rewards) / len(episode_rewards), global_step)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "        for rollout_data in rollout_buffer.get(batch_size):\n",
    "            actions = rollout_data.actions\n",
    "\n",
    "            values, log_prob, entropy = model.evaluate_actions(rollout_data.observations, actions)\n",
    "            values = values.flatten()\n",
    "\n",
    "            # Normalize advantage\n",
    "            advantages = rollout_data.advantages\n",
    "            if normalize_advantage:\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # ratio between old and new policy, should be one at the first iteration\n",
    "            ratio = torch.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            policy_loss_1 = advantages * ratio\n",
    "            policy_loss_2 = advantages * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "            policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "            # Value loss using the TD(gae_lambda) target\n",
    "            value_loss = F.mse_loss(rollout_data.returns, values)\n",
    "\n",
    "            # Entropy loss favor exploration\n",
    "            entropy_loss = -torch.mean(entropy)\n",
    "\n",
    "            loss = policy_loss + ent_coef * entropy_loss + vf_coef * value_loss\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip grad norm\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logs\n",
    "            writer.add_scalar(\"train/policy_loss\", policy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/value_loss\", value_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/entropy_loss\", entropy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, \"checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_con",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
